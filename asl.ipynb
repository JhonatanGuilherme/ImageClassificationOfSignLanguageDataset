{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Theory Behind Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a *simpler image classification example*, we need to load and visualize our data, edit our data (reshape, normalize, transform to categorical), create our model, compile our model and train the model on our data.\n",
    "\n",
    "In the *layer size*, we set a number of inputs and add some layers of neurons (or nodes), and an outuput layer of `x` neurons (one for each digit).\n",
    "\n",
    "Every neuron in each layer is connected to every neuron in the next layer. This is what the “Dense” Keras layer means. That can store a whole lot of information.\n",
    "\n",
    "So if each neuron has a connection between the oher, how do we store information? We store it in the “weights” between two neurons. This is the analogous to the strength of connection between the neurons of our brain (although with some differences in the mechanisms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **A Simpler Model**\n",
    "    \n",
    "    The earliest use case of one neuron was to build a **regression line**, using the equation $y = mx + b$. **Regression is when we use a continues input to predict a continuous output**.\n",
    "    \n",
    "    For each variable coming into our neuron, we’re going to find a **slope** ($m$), or *“weight”* for it. The $b$, or *y-intercept*, from $y=mx+b$ is stored with the neuron. Our goal here is to **find a line that can go through these points**. That way, we can use that line to make predictions.\n",
    "    \n",
    "    Traditional regression lines are built on the concept of **least squared error**, meaning, we’re going to take the difference of each estimate from the true value and square it.\n",
    "    \n",
    "    ```python\n",
    "    def get_rmse(data, m, b):\n",
    "    \t\"\"\"Calculates Mean Square Error.\"\"\"\n",
    "    \tn = len(data)\n",
    "    \tsquared_error = 0\n",
    "    \tfor x, y in data:\n",
    "    \t\t# Find predicted y\n",
    "    \t\ty_hat = m * x + b\n",
    "    \t\t# Square difference between prediction and true value\n",
    "    \t\tsquared_error += (y - y_hat) ** 2\n",
    "    \tmse = squared_error / n\n",
    "    \treturn mse ** .5\n",
    "    ```\n",
    "    \n",
    "    To get the **loss curve**, we have to use:\n",
    "    \n",
    "    - **The Gradient**: Gradient Algorithm, to calculate which direction loss decreases the most.\n",
    "    - $**λ$ - The learning rate**: how far to travel.\n",
    "    - **Epoch**: A model update with the full dataset.\n",
    "    - **Batch**: A sample of the full dataset.\n",
    "    - **Step**: An update to the weight parameters.\n",
    "    \n",
    "    A lot of research has been done on the best way to define the learning rate, and machine learning frameworks have a few tools that will automatically adjust the learning rate.\n",
    "    \n",
    "    > For instance, a popular one is **Adam (Adaptive momentum)** which is kind of like thinking of our loss curve as a mountain and our position as a marble. If we drop a marble on top of a mountain, it will pick up speed, jumping over trenches (local minima) before hopefully landing at a lower minima. Adam has since been improved, and the current default in TensorFlow is called “**RMSprop**”.\n",
    "    > \n",
    "2. **From Neuron to Network**\n",
    "    \n",
    "    After explaining a few of deep learning concepts, we can start building up our network. Instead of having a singular `x` , we’re going to have to have multiple `x`  inputs and find each of them a **weigth**. We just need to find the gradient for the new variable.\n",
    "    \n",
    "    We can also take the output and feed it into another, and as long as we don’t make a loop, we can connect the same output to multiple inputs. And with this we officially have a **deep learning network**. When we calculate from gradient descent, we can use the error calculated in a lter neuron as part of the error for the previous neuron it’s connected to.\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    > **Backpropagation algorithm** - Is essential for training large neural networks quickly. The activation function has to be a non-linear function, otherwise the neural network will only be able to learn linear models.\n",
    "    > \n",
    "    \n",
    "    > A commonly used activation function is the **Sigmoid function**: $f(x)=\\frac{1}{1+e^{⁻x}}$.\n",
    "    > \n",
    "    \n",
    "    > The goal is to l**earn the weights of the network automatically from data** such that the predicted output $y_{output}$  is close to the target $y_{target}$ for all inputs $x_{input}$.\n",
    "    > \n",
    "    \n",
    "    ---\n",
    "    \n",
    "3. **Activation Functions**\n",
    "    \n",
    "    There are a lot of activation functions, some more used are:\n",
    "    \n",
    "    1. **Linear**\n",
    "        - $ŷ=wx + b$\n",
    "        \n",
    "        ```python\n",
    "        # Multiply each input\n",
    "        # with a weight (w) and\n",
    "        # and intercept (b)\n",
    "        y_hat = w * x + b\n",
    "        ```\n",
    "        \n",
    "    2. **ReLU (rectified linear activation function)**\n",
    "        - $ŷ=\\begin{cases} wx + b &if& wx+b > 0 \\\\ 0 &otherwise& \\end{cases}$\n",
    "        \n",
    "        ```python\n",
    "        # Only return result\n",
    "        # if total is positive\n",
    "        linear = w * x + b\n",
    "        y_hat = linear * (linear > 0)\n",
    "        ```\n",
    "        \n",
    "    3. **Sigmoid**\n",
    "        - $ŷ=\\frac{1}{1 + e ^ {-(wx+b)}}$\n",
    "        \n",
    "        ```python\n",
    "        # Start with line\n",
    "        linear = w * x + b\n",
    "        # Warp to - inf to 0\n",
    "        inf_to_zero = np.exp(-1 * linear)\n",
    "        # Squish to -1 to 1\n",
    "        y_hat = 1 / (1 + inf_to_zero)\n",
    "        ```\n",
    "        \n",
    "    \n",
    "    Computer like equations of a line because they’re quick to compute and it’s easy for us to give it the rules on how to differentiate them.\n",
    "    \n",
    "    One easy way to add non-linearity is to feed our equation of a line into another non-linear function.\n",
    "    \n",
    "    Given enough data, our neural network will figure our the parameters of each of these-sub-components for us. Having a general understanding of the shape of the data and the relationship between the variables can help us build more efficient models for our data, saving time and computation.\n",
    "    \n",
    "4. **Overfitting**\n",
    "    \n",
    "    Why not have a super large neural network?\n",
    "    \n",
    "    - The problem goes back to classical statistics, but it still plagues neural networks.\n",
    "    - Not all problems can be so simple, and it’s the job of a Data Scientist to be able to determine the correct complexity for the model given.\n",
    "\n",
    "5. **From neuron to classification**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
